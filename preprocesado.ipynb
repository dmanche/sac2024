{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "\n",
    "print(tf.__version__) # La versión utilizada fue la 2.10.0, compatible con python 3.8.0\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos sin ruido\n",
    "root_path = r\"C:\\Users\\danie\\OneDrive\\Escritorio\\TFG\\TFG_entorno\\datos_sinteticos_amass_definitivos\"\n",
    "\n",
    "pattern = os.path.join(root_path, '**', '*.pkl')\n",
    "\n",
    "accumulated_data = []\n",
    "\n",
    "for file_path in glob.glob(pattern, recursive=True):\n",
    "    with open(file_path, 'rb') as file:\n",
    "\n",
    "        data = pickle.load(file)\n",
    "\n",
    "        if data['gt'] != 'unknown': # Si queremos todos los datos, eliminar esta línea\n",
    "            accumulated_data.append(data)\n",
    "\n",
    "print(f'Se han cargado los datos de {len(accumulated_data)} archivos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el número de frames por clase\n",
    "def calcular_total_frames(secuencias):\n",
    "    return sum(len(secuencia['acc']) for secuencia in secuencias)\n",
    "\n",
    "numero_stand = calcular_total_frames([secuencia for secuencia in accumulated_data if secuencia['gt'] == 'stand'])\n",
    "numero_walk = calcular_total_frames([secuencia for secuencia in accumulated_data if secuencia['gt'] == 'walk'])\n",
    "numero_run = calcular_total_frames([secuencia for secuencia in accumulated_data if secuencia['gt'] == 'run'])\n",
    "numero_sit = calcular_total_frames([secuencia for secuencia in accumulated_data if secuencia['gt'] == 'sit'])\n",
    "\n",
    "numero_frames = numero_stand + numero_walk + numero_run + numero_sit\n",
    "\n",
    "print(f'El número de frames de stand es {numero_stand}.')\n",
    "print(f'El número de frames de walk es {numero_walk}.')\n",
    "print(f'El número de frames de run es {numero_run}.')\n",
    "print(f'El número de frames de sit es {numero_sit}.')\n",
    "print(f'El número total de frames es {numero_frames}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceamos las cuatro clases\n",
    "\n",
    "# num_objetivo_frames es el número de frames deseado, ajustado al mínimo de las cuatro clases\n",
    "num_objetivo_frames = min(numero_stand, numero_walk, numero_run, numero_sit)\n",
    "\n",
    "# Separar secuencias por clase\n",
    "secuencias_por_clase = {'walk': [], 'run': [], 'sit': [], 'stand': []}\n",
    "for secuencia in accumulated_data:\n",
    "    secuencias_por_clase[secuencia['gt']].append(secuencia)\n",
    "\n",
    "# Función para calcular el total de frames de una lista de secuencias\n",
    "def calcular_total_frames(secuencias):\n",
    "    return sum(len(secuencia['acc']) for secuencia in secuencias)\n",
    "\n",
    "# Balancear los datos para cada clase\n",
    "np.random.seed(13)  # Para reproducibilidad\n",
    "clases_para_balancear = ['walk', 'run', 'sit', 'stand']\n",
    "\n",
    "frames_total_balanceados = 0\n",
    "\n",
    "for clase in clases_para_balancear:\n",
    "    while calcular_total_frames(secuencias_por_clase[clase]) > num_objetivo_frames:\n",
    "        # Elegir y eliminar una secuencia aleatoria\n",
    "        del_index = np.random.choice(range(len(secuencias_por_clase[clase])))\n",
    "        del secuencias_por_clase[clase][del_index]\n",
    "    frames_total_balanceados += calcular_total_frames(secuencias_por_clase[clase])\n",
    "\n",
    "# Reconstruir la lista de secuencias balanceadas con la nueva estructura deseada\n",
    "secuencias_balanceadas = []\n",
    "for secuencias in secuencias_por_clase.values():\n",
    "    for secuencia in secuencias:\n",
    "        # Crear un nuevo diccionario para cada secuencia reorganizada\n",
    "        new_secuencia = {\n",
    "            'acc': [frame for frame in secuencia['acc']],\n",
    "            'ori': [frame for frame in secuencia['ori']],\n",
    "            'gt': secuencia['gt']  # Asociar 'gt' una sola vez por secuencia, no por frame\n",
    "        }\n",
    "        secuencias_balanceadas.append(new_secuencia)\n",
    "\n",
    "print(f'El número total de frames balanceados es {frames_total_balanceados}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_data = secuencias_balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aislamos los datos en tres listas diferentes\n",
    "gt = [data['gt'] for data in accumulated_data]\n",
    "acc = [data['acc'] for data in accumulated_data]\n",
    "ori = [data['ori'] for data in accumulated_data]\n",
    "\n",
    "del accumulated_data\n",
    "accumulated_data = [acc, ori]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accumulated_data[0][0][0].shape) # [acc/ori][sample num][frame num][IMU]\n",
    "print(accumulated_data[1][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_datos(datos):\n",
    "    datos_aceleracion = datos[0]\n",
    "    datos_orientacion = datos[1]\n",
    "    \n",
    "    n_imus = datos_aceleracion[0][0].shape[0]  # Número de IMUs\n",
    "    \n",
    "    # Determinar la longitud máxima de las secuencias\n",
    "    max_length = max(max(len(grabacion) for grabacion in datos_aceleracion), max(len(grabacion) for grabacion in datos_orientacion))\n",
    " \n",
    "    # Calcular el número total de características por frame después del aplanamiento\n",
    "    num_features_aceleracion = n_imus * 3\n",
    "    num_features_orientacion = n_imus * 3 * 3\n",
    "    total_features = num_features_aceleracion + num_features_orientacion\n",
    "    \n",
    "    # Preparar el array para los datos con padding\n",
    "    datos_rnn_con_padding = np.zeros((len(datos_aceleracion), max_length, total_features))\n",
    "    \n",
    "    for i in range(len(datos_aceleracion)):\n",
    "        grabacion_aceleracion = datos_aceleracion[i]\n",
    "        grabacion_orientacion = datos_orientacion[i]\n",
    "        \n",
    "        for j in range(min(len(grabacion_aceleracion), len(grabacion_orientacion))):\n",
    "            # Aplanar los datos de aceleración y orientación\n",
    "            aceleracion_aplanada = grabacion_aceleracion[j].reshape(-1)\n",
    "            orientacion_aplanada = grabacion_orientacion[j].reshape(-1)\n",
    "            \n",
    "            # Concatenar aceleración y orientación aplanadas\n",
    "            frame_aplanado = np.concatenate([aceleracion_aplanada, orientacion_aplanada])\n",
    "            \n",
    "            # Asignar al array de resultados\n",
    "            datos_rnn_con_padding[i, j, :] = frame_aplanado\n",
    "    \n",
    "    return datos_rnn_con_padding\n",
    "\n",
    "accumulated_data_rnn = preparar_datos(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el One Hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Primero, convertimos las etiquetas categóricas a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "gt_int = label_encoder.fit_transform(gt)\n",
    "\n",
    "# Luego, convertimos los números enteros a codificación one-hot\n",
    "gt_one_hot = to_categorical(gt_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gt_one_hot[0]))\n",
    "print(gt_one_hot[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extendemos las etiquetas de cada secuencia a cada uno de los frames\n",
    "gt_categorical_extended = []\n",
    "\n",
    "for i, frames in enumerate(acc):\n",
    "    # Cantidad de frames en la grabación actual\n",
    "    num_frames = len(accumulated_data_rnn[i])\n",
    "    \n",
    "    # Utiliza numpy.tile para repetir la etiqueta categórica para cada frame\n",
    "    gt_repeated = np.tile(gt_one_hot[i], (num_frames, 1))\n",
    "    \n",
    "    gt_categorical_extended.append(gt_repeated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la lista de arrays en un único array de NumPy\n",
    "Y = np.array(gt_categorical_extended)\n",
    "#Y = np.array(gt_one_hot) #Cambiar cuando se use CNN-1D\n",
    "# Verificar la forma de Y para confirmar que es correcta\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(accumulated_data_rnn, Y, test_size = 0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {Y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"modelo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(f\"Accuracy en los datos de prueba: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1) #Cambiar a 1 cuando se clasifique por secuencias en lugar de por frames\n",
    "y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.flatten(), y_pred.flatten(), target_names=label_encoder.classes_))\n",
    "\n",
    "# Matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.flatten(), y_pred.flatten())\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
